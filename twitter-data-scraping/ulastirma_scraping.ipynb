{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a639e7c-4d9a-4e60-a9bc-d2d8d5a8e53c",
   "metadata": {},
   "source": [
    "dog03466@nezid.com 5d5894e73c2425f92199d8daaf62105ac80dbe44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173df048-be78-4254-b184-7b45a0f6362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/Eren/Desktop/twitter otomasyon/ailesosyal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33e5c01-332a-45aa-b175-52bec9616210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is F895-A173\n",
      "\n",
      " Directory of C:\\Users\\Eren\\Desktop\\twitter otomasyon\\ailesosyal\n",
      "\n",
      "02.03.2024  10:58    <DIR>          .\n",
      "02.03.2024  10:38    <DIR>          ..\n",
      "02.03.2024  10:38    <DIR>          .ipynb_checkpoints\n",
      "02.03.2024  10:56    <DIR>          datatcailesosyal\n",
      "02.03.2024  10:58            23.718 Untitled.ipynb\n",
      "               1 File(s)         23.718 bytes\n",
      "               4 Dir(s)  45.919.944.704 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0021ecd-c890-41c1-86c6-3015aff04ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\eren\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\eren\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\eren\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\eren\\anaconda3\\lib\\site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\eren\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for matplotlib: [Errno 2] No such file or directory: 'c:\\\\users\\\\eren\\\\anaconda3\\\\lib\\\\site-packages\\\\matplotlib-3.7.1.dist-info\\\\METADATA'\n",
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!curl -sL https://deb.nodesource.com/setup_18.x | sudo -E bash -\n",
    "!sudo apt-get install -y nodejs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92690f8a-3b51-42d8-94b2-dea50d275828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data for 2023-10-26: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-27.csv'\n",
      "Error loading data for 2023-10-25: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-26.csv'\n",
      "Error loading data for 2023-10-24: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-25.csv'\n",
      "Error loading data for 2023-10-23: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-24.csv'\n",
      "Error loading data for 2023-10-22: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-23.csv'\n",
      "Error loading data for 2023-10-21: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-22.csv'\n",
      "Error loading data for 2023-10-20: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-21.csv'\n",
      "Error loading data for 2023-10-19: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-20.csv'\n",
      "Error loading data for 2023-10-18: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-19.csv'\n",
      "Error loading data for 2023-10-17: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-18.csv'\n",
      "Error loading data for 2023-10-16: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-17.csv'\n",
      "Error loading data for 2023-10-15: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-16.csv'\n",
      "Error loading data for 2023-10-14: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-15.csv'\n",
      "Error loading data for 2023-10-13: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-14.csv'\n",
      "Error loading data for 2023-10-12: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-13.csv'\n",
      "Error loading data for 2023-10-11: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-12.csv'\n",
      "Error loading data for 2023-10-10: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-11.csv'\n",
      "Error loading data for 2023-10-09: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-10.csv'\n",
      "Error loading data for 2023-10-08: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-09.csv'\n",
      "Error loading data for 2023-10-07: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-08.csv'\n",
      "Error loading data for 2023-10-06: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-07.csv'\n",
      "Error loading data for 2023-10-05: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-06.csv'\n",
      "Error loading data for 2023-10-04: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-05.csv'\n",
      "Error loading data for 2023-10-03: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-04.csv'\n",
      "Error loading data for 2023-10-02: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-03.csv'\n",
      "Error loading data for 2023-10-01: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-02.csv'\n",
      "Error loading data for 2023-09-30: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-10-01.csv'\n",
      "Error loading data for 2023-09-29: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-30.csv'\n",
      "Error loading data for 2023-09-28: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-29.csv'\n",
      "Error loading data for 2023-09-27: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-28.csv'\n",
      "Error loading data for 2023-09-26: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-27.csv'\n",
      "Error loading data for 2023-09-25: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-26.csv'\n",
      "Error loading data for 2023-09-24: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-25.csv'\n",
      "Error loading data for 2023-09-23: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-24.csv'\n",
      "Error loading data for 2023-09-22: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-23.csv'\n",
      "Error loading data for 2023-09-21: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-22.csv'\n",
      "Error loading data for 2023-09-20: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-21.csv'\n",
      "Error loading data for 2023-09-19: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-20.csv'\n",
      "Error loading data for 2023-09-18: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-19.csv'\n",
      "Error loading data for 2023-09-17: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-18.csv'\n",
      "Error loading data for 2023-09-16: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-17.csv'\n",
      "Error loading data for 2023-09-15: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-16.csv'\n",
      "Error loading data for 2023-09-14: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-15.csv'\n",
      "Error loading data for 2023-09-13: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-14.csv'\n",
      "Error loading data for 2023-09-12: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-13.csv'\n",
      "Error loading data for 2023-09-11: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-12.csv'\n",
      "Error loading data for 2023-09-10: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-11.csv'\n",
      "Error loading data for 2023-09-09: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-10.csv'\n",
      "Error loading data for 2023-09-08: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-09.csv'\n",
      "Error loading data for 2023-09-07: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-08.csv'\n",
      "Error loading data for 2023-09-06: [Errno 2] No such file or directory: '/content/drive/MyDrive/otomasyon_tweet/tweets-data/datatcailesosyal/tcailesosyal_2023-09-07.csv'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# tweet-harvest komutunu çalıştır\u001b[39;00m\n\u001b[0;32m     18\u001b[0m command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpx --yes tweet-harvest@2.2.8 -o \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m -s \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_keyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m -l \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m --token \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5d5894e73c2425f92199d8daaf62105ac80dbe44\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Bir sonraki güne geç\u001b[39;00m\n\u001b[0;32m     22\u001b[0m start_date \u001b[38;5;241m=\u001b[39m next_day\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Başlangıç ve bitiş tarihleri\n",
    "start_date = date(2023, 10, 27)\n",
    "end_date = date(2020, 1, 1)\n",
    "limit = 500\n",
    "\n",
    "while start_date >= end_date:\n",
    "    # Her gün için tweet toplama\n",
    "    next_day = start_date - timedelta(days=1)\n",
    "    data = f\"datatcailesosyal/tcailesosyal_{start_date}.csv\"\n",
    "    search_keyword = f\"to:@tcailesosyal until:{start_date} since:{next_day}\"\n",
    "\n",
    "    # tweet-harvest komutunu çalıştır\n",
    "    command = f\"npx --yes tweet-harvest@2.2.8 -o '{data}' -s '{search_keyword}' -l {limit} --token '5d5894e73c2425f92199d8daaf62105ac80dbe44'\"\n",
    "    os.system(command)\n",
    "\n",
    "    # Bir sonraki güne geç\n",
    "    start_date = next_day\n",
    "\n",
    "    # Her 300 tweet'te bir 30 saniye bekle\n",
    "\n",
    "\n",
    "    # Yüklenen veri setini kontrol et\n",
    "    try:\n",
    "        df = pd.read_csv(f\"/content/drive/MyDrive/otomasyon_tweet/tweets-data/{data}\", sep=\";\")\n",
    "        print(f\"Data loaded for {start_date}, shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {start_date}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f2316-93aa-4cb3-bf12-1a8eff484cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4a9b4-935c-45eb-8c81-bf0793f9819d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data for 2023-10-26: [Errno 2] No such file or directory: 'Users/Eren/Desktop/twitter otomasyon/ailesosyal\\\\ailesosyal5/tcailesosyal_2023-10-27.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Dosyaların kaydedileceği ve okunacağı temel yol\n",
    "base_path = 'Users/Eren/Desktop/twitter otomasyon/ailesosyal'\n",
    "\n",
    "# Başlangıç ve bitiş tarihleri\n",
    "start_date = date(2023, 10, 27)\n",
    "end_date = date(2020, 1, 1)\n",
    "limit = 500\n",
    "\n",
    "while start_date >= end_date:\n",
    "    # Her gün için tweet toplama\n",
    "    next_day = start_date - timedelta(days=1)\n",
    "    data_filename = f\"ailesosyal5/tcailesosyal_{start_date}.csv\"\n",
    "    data_path = os.path.join(base_path, data_filename)  # Dosyanın tam yolu\n",
    "    search_keyword = f\"to:@tcailesosyal until:{start_date} since:{next_day}\"\n",
    "\n",
    "    # tweet-harvest komutunu çalıştır\n",
    "    command = f\"npx --yes tweet-harvest@2.2.8 -o '{data_path}' -s '{search_keyword}' -l {limit} --token '5d5894e73c2425f92199d8daaf62105ac80dbe44'\"\n",
    "    try:\n",
    "        subprocess.run(command, check=True, shell=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command '{command}' failed with error: {e}\")\n",
    "\n",
    "    # Bir sonraki güne geç\n",
    "    start_date = next_day\n",
    "\n",
    "    # Yüklenen veri setini kontrol et\n",
    "    try:\n",
    "        df = pd.read_csv(data_path, sep=\";\")\n",
    "        print(f\"Data loaded for {start_date}, shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {start_date}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207cdac-03bb-4242-8db7-b73fe39e62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Dosyaların kaydedileceği ve okunacağı temel yol\n",
    "base_path = '/Users/Eren/Desktop/twitter otomasyon/ailesosyal'\n",
    "\n",
    "# Başlangıç ve bitiş tarihleri\n",
    "start_date = date(2023, 10, 27)\n",
    "end_date = date(2023, 10, 25)\n",
    "limit = 500\n",
    "\n",
    "while start_date >= end_date:\n",
    "    # Her gün için tweet toplama\n",
    "    next_day = start_date - timedelta(days=1)\n",
    "    data_filename = f\"tcailesosyal_{start_date}.csv\"\n",
    "    # Ensure the path is correctly formatted for Windows\n",
    "    data_path = os.path.join(base_path, data_filename).replace('/', '\\\\')\n",
    "    search_keyword = f\"to:@tcailesosyal until:{start_date} since:{next_day}\"\n",
    "\n",
    "    # Prepare the command separately to avoid syntax issues with backslashes\n",
    "    command = \"npx --yes tweet-harvest@2.2.8 -o \\\"\" + data_path + \"\\\" -s \\\"\" + search_keyword + \"\\\" -l \" + str(limit) + \" --token '5d5894e73c2425f92199d8daaf62105ac80dbe44'\"\n",
    "\n",
    "    try:\n",
    "        subprocess.run(command, check=True, shell=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command '{command}' failed with error: {e}\")\n",
    "\n",
    "    # Bir sonraki güne geç\n",
    "    start_date = next_day\n",
    "\n",
    "    # Yüklenen veri setini kontrol et\n",
    "    try:\n",
    "        df = pd.read_csv(data_path, sep=\";\")\n",
    "        print(f\"Data loaded for {start_date}, shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {start_date}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "485aa4c1-1227-4921-ab5a-518c71475b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '17.csv'\n",
    "search_keyword = 'terörü lanetliyorum until:2022-09-11 since:2020-11-01'\n",
    "limit = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44908133-1fd2-425c-b1f6-d8ef75a2e218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22m\n",
      "\u001b[1mWelcome to the Twitter Crawler ğŸ•·ï¸�\u001b[22m\n",
      "\u001b[1m\u001b[22m\n",
      "This script uses Chromium Browser to crawl data from Twitter with *your* Twitter auth token.\n",
      "Please enter your Twitter auth token when prompted.\n",
      "\n",
      "Note: Keep your access token secret! Don't share it with anyone else.\n",
      "Note: This script only runs on your local device.\n",
      "\n",
      "\n",
      "up to date, audited 4 packages in 902ms\n",
      "\n",
      "found 0 vulnerabilities\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mOpening twitter search page...\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mFound existing file ./tweets-data/17.csv, renaming to ./tweets-data/17.old.csv\u001b[39m\n",
      "\u001b[33m\u001b[39m\n",
      "\u001b[33mFilling in keywords: to:@UABakanligi until:2022-08-07 since:2018-01-01\u001b[39m\n",
      "\u001b[33m\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mGot some tweets, saving to file...\u001b[39m\n",
      "\u001b[34mYour tweets saved to: C:\\Users\\Eren\\Desktop\\twitter otomasyon\\ailesosyal\\tweets-data\\17.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 12\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mGot some tweets, saving to file...\u001b[39m\n",
      "\u001b[34mYour tweets saved to: C:\\Users\\Eren\\Desktop\\twitter otomasyon\\ailesosyal\\tweets-data\\17.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 28\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mGot some tweets, saving to file...\u001b[39m\n",
      "\u001b[34mYour tweets saved to: C:\\Users\\Eren\\Desktop\\twitter otomasyon\\ailesosyal\\tweets-data\\17.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 40\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mGot some tweets, saving to file...\u001b[39m\n",
      "\u001b[34mYour tweets saved to: C:\\Users\\Eren\\Desktop\\twitter otomasyon\\ailesosyal\\tweets-data\\17.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 53\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mGot some tweets, saving to file...\u001b[39m\n",
      "\u001b[34mYour tweets saved to: C:\\Users\\Eren\\Desktop\\twitter otomasyon\\ailesosyal\\tweets-data\\17.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 68\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mGot some tweets, saving to file...\u001b[39m\n",
      "\u001b[34mYour tweets saved to: C:\\Users\\Eren\\Desktop\\twitter otomasyon\\ailesosyal\\tweets-data\\17.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 83\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mGot some tweets, saving to file...\u001b[39m\n",
      "\u001b[34mYour tweets saved to: C:\\Users\\Eren\\Desktop\\twitter otomasyon\\ailesosyal\\tweets-data\\17.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 96\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mGot some tweets, saving to file...\u001b[39m\n",
      "\u001b[34mYour tweets saved to: C:\\Users\\Eren\\Desktop\\twitter otomasyon\\ailesosyal\\tweets-data\\17.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 110\u001b[39m\n",
      "\u001b[90m\u001b[39m\n",
      "\u001b[90m--Taking a break, waiting for 10 seconds...\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mGot some tweets, saving to file...\u001b[39m\n",
      "\u001b[34mYour tweets saved to: C:\\Users\\Eren\\Desktop\\twitter otomasyon\\ailesosyal\\tweets-data\\17.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 118\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mGot some tweets, saving to file...\u001b[39m\n",
      "\u001b[34mYour tweets saved to: C:\\Users\\Eren\\Desktop\\twitter otomasyon\\ailesosyal\\tweets-data\\17.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 125\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mGot some tweets, saving to file...\u001b[39m\n",
      "Already got 125 tweets, done scrolling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No more tweets found, please check your search criteria and csv file result\n"
     ]
    }
   ],
   "source": [
    "!npx --yes tweet-harvest@2.2.8 -o \"{data}\" -s \"to:@UABakanligi until:2022-08-07 since:2018-01-01\" -l {limit} --token \"5d5894e73c2425f92199d8daaf62105ac80dbe44\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c6cc32-e0c4-454b-8b42-53ccfa172f22",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2559737841.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    base_path = \"C:\\Users\\Eren\\Desktop\\twitter\"\u001b[0m\n\u001b[1;37m                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Dosyaların kaydedileceği ve okunacağı temel yol\n",
    "base_path = \"C:\\Users\\Eren\\Desktop\\twitter\"\n",
    "\n",
    "# Başlangıç ve bitiş tarihleri\n",
    "start_date = date(2023, 9, 27)\n",
    "end_date = date(2023, 9, 25)\n",
    "limit = 500\n",
    "\n",
    "while start_date >= end_date:\n",
    "    # Her gün için tweet toplama\n",
    "    next_day = start_date - timedelta(days=1)\n",
    "    data_filename = f\"tcailesosyal_{start_date}.csv\"\n",
    "    data_path = os.path.join(base_path, data_filename)  # Dosyanın tam yolu\n",
    "    search_keyword = f\"to:@tcailesosyal until:{start_date} since:{next_day}\"\n",
    "\n",
    "    # tweet-harvest komutunu çalıştır\n",
    "    command = f\"npx --yes tweet-harvest@2.2.8 -o '{data_path}' -s '{search_keyword}' -l {limit} --token '5d5894e73c2425f92199d8daaf62105ac80dbe44'\"\n",
    "    try:\n",
    "        subprocess.run(command, check=True, shell=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Command '{command}' failed with error: {e}\")\n",
    "\n",
    "    # Bir sonraki güne geç\n",
    "    start_date = next_day\n",
    "\n",
    "    # Yüklenen veri setini kontrol et\n",
    "    try:\n",
    "        df = pd.read_csv(data_path, sep=\";\")\n",
    "        print(f\"Data loaded for {start_date}, shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {start_date}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e56e19-6631-4c46-b6e2-70105ec9d183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on 2022-08-01: Command '['\\x1b[1m\\x1b[22m', '\\x1b[1mWelcome to the Twitter Crawler ğŸ•·ï¸�\\x1b[22m', '\\x1b[1m\\x1b[22m', 'This script uses Chromium Browser to crawl data from Twitter with *your* Twitter auth token.', 'Please enter your Twitter auth token when prompted.', '', \"Note: Keep your access token secret! Don't share it with anyone else.\", 'Note: This script only runs on your local device.', '', '\\x1b[2K\\x1b[1G\\x1b[36m?\\x1b[39m \\x1b[1mHow many tweets do you want to crawl?\\x1b[22m \\x1b[90mÂ»\\x1b[39m \\x1b[36m\\x1b[4m\\x1b[90m\\x1b[39m\\x1b[36m\\x1b[39m\\x1b[24m\\x1b7\\x1b8']' returned non-zero exit status 1.\n",
      "Error on 2022-08-02: Command '['\\x1b[1m\\x1b[22m', '\\x1b[1mWelcome to the Twitter Crawler ğŸ•·ï¸�\\x1b[22m', '\\x1b[1m\\x1b[22m', 'This script uses Chromium Browser to crawl data from Twitter with *your* Twitter auth token.', 'Please enter your Twitter auth token when prompted.', '', \"Note: Keep your access token secret! Don't share it with anyone else.\", 'Note: This script only runs on your local device.', '', '\\x1b[2K\\x1b[1G\\x1b[36m?\\x1b[39m \\x1b[1mHow many tweets do you want to crawl?\\x1b[22m \\x1b[90mÂ»\\x1b[39m \\x1b[36m\\x1b[4m\\x1b[90m\\x1b[39m\\x1b[36m\\x1b[39m\\x1b[24m\\x1b7\\x1b8']' returned non-zero exit status 1.\n",
      "Error on 2022-08-03: Command '['\\x1b[1m\\x1b[22m', '\\x1b[1mWelcome to the Twitter Crawler ğŸ•·ï¸�\\x1b[22m', '\\x1b[1m\\x1b[22m', 'This script uses Chromium Browser to crawl data from Twitter with *your* Twitter auth token.', 'Please enter your Twitter auth token when prompted.', '', \"Note: Keep your access token secret! Don't share it with anyone else.\", 'Note: This script only runs on your local device.', '', '\\x1b[2K\\x1b[1G\\x1b[36m?\\x1b[39m \\x1b[1mHow many tweets do you want to crawl?\\x1b[22m \\x1b[90mÂ»\\x1b[39m \\x1b[36m\\x1b[4m\\x1b[90m\\x1b[39m\\x1b[36m\\x1b[39m\\x1b[24m\\x1b7\\x1b8']' returned non-zero exit status 1.\n",
      "Error on 2022-08-04: Command '['\\x1b[1m\\x1b[22m', '\\x1b[1mWelcome to the Twitter Crawler ğŸ•·ï¸�\\x1b[22m', '\\x1b[1m\\x1b[22m', 'This script uses Chromium Browser to crawl data from Twitter with *your* Twitter auth token.', 'Please enter your Twitter auth token when prompted.', '', \"Note: Keep your access token secret! Don't share it with anyone else.\", 'Note: This script only runs on your local device.', '', '\\x1b[2K\\x1b[1G\\x1b[36m?\\x1b[39m \\x1b[1mHow many tweets do you want to crawl?\\x1b[22m \\x1b[90mÂ»\\x1b[39m \\x1b[36m\\x1b[4m\\x1b[90m\\x1b[39m\\x1b[36m\\x1b[39m\\x1b[24m\\x1b7\\x1b8']' returned non-zero exit status 1.\n",
      "Error on 2022-08-05: Command '['\\x1b[1m\\x1b[22m', '\\x1b[1mWelcome to the Twitter Crawler ğŸ•·ï¸�\\x1b[22m', '\\x1b[1m\\x1b[22m', 'This script uses Chromium Browser to crawl data from Twitter with *your* Twitter auth token.', 'Please enter your Twitter auth token when prompted.', '', \"Note: Keep your access token secret! Don't share it with anyone else.\", 'Note: This script only runs on your local device.', '', '\\x1b[2K\\x1b[1G\\x1b[36m?\\x1b[39m \\x1b[1mHow many tweets do you want to crawl?\\x1b[22m \\x1b[90mÂ»\\x1b[39m \\x1b[36m\\x1b[4m\\x1b[90m\\x1b[39m\\x1b[36m\\x1b[39m\\x1b[24m\\x1b7\\x1b8']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import subprocess\n",
    "\n",
    "# Başlangıç ve bitiş tarihlerini belirle\n",
    "start_date = datetime(2022, 8, 1)\n",
    "end_date = datetime(2022, 8, 7)\n",
    "\n",
    "# Token'ınızı bir değişkene atayın\n",
    "token = \"5d5894e73c2425f92199d8daaf62105ac80dbe44\"\n",
    "\n",
    "# Belirtilen tarih aralığı için her gün için döngü yap\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    # Dosya adını tarihle etiketle\n",
    "    data = current_date.strftime('%Y-%m-%d') + '.csv'\n",
    "    \n",
    "    # Arama anahtar kelimesini ayarla\n",
    "    next_day = current_date + timedelta(days=1)\n",
    "    search_keyword = f\"to:@UABakanligi until:{next_day.strftime('%Y-%m-%d')} since:{current_date.strftime('%Y-%m-%d')}\"\n",
    "    \n",
    "    # Komutu oluştur\n",
    "    command = f\"npx --yes tweet-harvest@2.2.8 -o '{data}' -s '{search_keyword}' -l {limit} --token '{token}'\"\n",
    "    \n",
    "    # Komutu çalıştır\n",
    "    try:\n",
    "        subprocess.run(command, check=True, shell=True)\n",
    "        print(f\"Data collected for {current_date.strftime('%Y-%m-%d')}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error on {current_date.strftime('%Y-%m-%d')}: {e}\")\n",
    "    \n",
    "    # Bir sonraki güne geç\n",
    "    current_date = next_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e4f6b78-45bf-4d7b-a767-873ef1868958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import subprocess\n",
    "\n",
    "# Başlangıç ve bitiş tarihlerini ayarla\n",
    "baslangic_tarihi = datetime.strptime(\"2022-08-05\", \"%Y-%m-%d\")\n",
    "bitis_tarihi = datetime.strptime(\"2022-08-07\", \"%Y-%m-%d\")\n",
    "\n",
    "# Her gün için bir döngü başlat\n",
    "current_date = baslangic_tarihi\n",
    "while current_date <= bitis_tarihi:\n",
    "    # Dosya adını ve tarih aralığını belirle\n",
    "    dosya_adi = current_date.strftime(\"%Y-%m-%d\") + \".csv\"\n",
    "    tarih_araligi = f\"since:{current_date.strftime('%Y-%m-%d')} until:{(current_date + timedelta(days=1)).strftime('%Y-%m-%d')}\"\n",
    "\n",
    "    # tweet-harvest komutunu oluştur\n",
    "    komut = f\"npx --yes tweet-harvest@2.2.8 -o \\\"{dosya_adi}\\\" -s \\\"to:@UABakanligi {tarih_araligi}\\\" -l 500 --token \\\"5d5894e73c2425f92199d8daaf62105ac80dbe44\\\"\"\n",
    "\n",
    "    # Komutu çalıştır\n",
    "    subprocess.run(komut, shell=True)\n",
    "\n",
    "    # Bir sonraki güne geç\n",
    "    current_date += timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b6b97-8090-4666-b5aa-991e6b36e725",
   "metadata": {},
   "source": [
    "**bu kod calısıyor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f98b7-b4d4-4ff5-a569-866936e69fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import subprocess\n",
    "\n",
    "# Başlangıç ve bitiş tarihlerini ayarla\n",
    "baslangic_tarihi = datetime.strptime(\"2015-01-01\", \"%Y-%m-%d\")\n",
    "bitis_tarihi = datetime.strptime(\"2016-01-01\", \"%Y-%m-%d\")\n",
    "\n",
    "# Her gün için bir döngü başlat\n",
    "current_date = baslangic_tarihi\n",
    "while current_date <= bitis_tarihi:\n",
    "    # Dosya adını ve tarih aralığını belirle\n",
    "    dosya_adi = \"Ulaştırma_\" + current_date.strftime(\"%Y-%m-%d\") + \".csv\"\n",
    "    tarih_araligi = f\"since:{current_date.strftime('%Y-%m-%d')} until:{(current_date + timedelta(days=1)).strftime('%Y-%m-%d')}\"\n",
    "\n",
    "    # tweet-harvest komutunu oluştur\n",
    "    komut = f\"npx --yes tweet-harvest@2.2.8 -o \\\"{dosya_adi}\\\" -s \\\"to:@UABakanligi {tarih_araligi}\\\" -l 500 --token \\\"31164c437fa691ab15b3bb3e474a796271fc5f80\\\"\"\n",
    "\n",
    "    # Komutu çalıştır\n",
    "    subprocess.run(komut, shell=True)\n",
    "\n",
    "    # Bir sonraki güne geç\n",
    "    current_date += timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e75181-8b71-406d-b373-9b3638de9956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b3b813-8bf4-4d78-99b9-0032ff909a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
